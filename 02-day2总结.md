Day 2 课程总结：

1.  block和block中的线程，只有先上到SM（流多处理器）内部，才可能开始执行。因此Ken老师强调说，每个block使用的资源少点，则1个SM上可能有更多的blocks（和其中的线程）在被调度执行中。注意这里的最后一句的活动warps. 在很多NV的文档场合，active和resident warps是混用的（都代表了驻留的warps。前者常见于nsight文档，后者常见于CUDA的文档），都是指上到了SM上的block中的（warps）。 

2.   注意像是c = a * 1.23456f + b; 这里的1.23456f是所有线程都一致的编译时刻常数，编译器会自动收集这种，并全自动的放入constant cache. 不需要手工放（例如通过幻灯片中的__constant__ 和 cudaMemcpyToSymbol） 

3.   2D或者3D存储上的“空间临近性".这点和普通的cache（线性地址，例如连续的64B之类的）不同。texture cache模拟的“空间上的局部性"缓冲。 

4.    注意在GPU上warp整体在并行，不是1个线程各自为战。在GPU编程中，总是以warp的角度去看事情的（32个线程）。所以是否真的按行，得从warp的角度看。 

5.     shared memory：（1）当成一个局部的小高速的交换数据的小Scratch buffer用。（2）当成一个服务多次重复数据访问的，手工管理的cache来用。（3）将直接访问global memory不合适的pattern，通过shared memory中转后，变成合适的（哪怕此时并没有显著的多次重复使用）。 

6.     很多GPU设备上，L1 cache和shared memory共享物理的存储单元（但我们本次课程的平台(Jetson)上不是。和L1 cache互相独立）。 

7.     shared memory有多个独立的bank，每个bank都可以独立工作。但不能同一周期内，有2个线程使用同1个bank（的不同深度的内容）。shared memory不使用虚拟内存（直接使用物理地址访问）

8.    当warp中的32个线程无冲突的访问多个banks的时候，这些banks可以同时工作，并行给出数据，性能很好。 

9.      为啥不是只读取完整的一行到share memory？ 而要读tile 行，然后截取列？

--我们的shared memory容量有限，无法直接容纳一个16*N（对于MxN * N*K的两个矩阵来说），那么巨大的公用共享空间。所以每次用一个小的（例如,16x16的小shared memory，然后反复重用这个小的，用完的数据就扔掉，再滑动）。 

10.  请问texture memory在这里是不是也适用呢？

--理论各种普通cache都可以考虑用，但是实际上shared的表现往往最好。所以本例使用shared（主要是超低延迟，行列转置之类的免费）这里比较好。 

11.  shared memory容量最大值可以确定或者获取到吗？还是每次都保险用16x16的小shared memory？

--可以获取到，目前所有你能买到的卡都至少48KB。但是使用16x16还有其他的原因。主要是尽量使用方块容易理解。而下一级的方块就是32x32了（最大1024），所以用16x16较好。而用8x8的方块又太小。 

12.  怎么判断有没有发生bank conflict？

--每行warp访问shared memory，你如果找不出来任意2个线程，能访问同1个bank的不同深度，那就没有。（或者用人话说，对于4B的元素构成的数组，1D化后的元素下标，你找不到2个线程能正好相邻32的倍数，那就没有），注意这是对每次warp访存shared来说的。多次之间无干扰。
 

13.  做shared memory进行矩阵乘法的例子，同学们不要迷惑了。block中的线程在集体合力读取内容到shared memory中的时候，是以block整体的访存的（坐标）的角度出发的。而当它们完成了写入，执行完了__syncthreads()后，再次就是各自为战负责1个点的坐标了（目标矩阵里的坐标）。这里面的坐标的身份有变化，大家不要迷惑了！ 

14.   学会了这个走shared memory的矩阵乘法，你会学会：（1）如何使用一种高速存储单元。（2）如果以warp整体的角度访存global的思考。（3）如何同一个kernel的代码中，前后片段中的每个线程，有不同的身份/工作。

15.     在我们申请的block中所有线程已经确定知道要去求P矩阵中的那个值的时候，它们的by已经确定了，那是线程自身的属性，不会变得。我们只是帮助线程去找它所需要的数据。